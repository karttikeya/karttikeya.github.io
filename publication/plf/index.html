<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="theme" content="hugo-academic">
  <meta name="generator" content="Hugo 0.49" />
  <meta name="author" content="Karttikeya Mangalam">
  <meta name="description" content="PhD Student in Computer Science">

  
  
  
    
  
  
    
    
    <link rel="stylesheet" href="/css/highlight.min.css">
    
  
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha512-6MXa8B6uaO18Hid6blRMetEIoPqHf7Ux1tnyIQdpt9qI5OACx7C+O3IVTr98vwGnlcg0LOLa02i9Y1HpVhlfiw==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.0/css/academicons.min.css" integrity="sha512-GGGNUPDhnG8LEAEDsjqYIQns+Gu8RBs4j5XGlxl7UfRaZBhCCm5jenJkeJL8uPuOXGqgl8/H1gjlWQDRjd3cUQ==" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha512-SfTiTlX6kk+qitfevl/7LibUOeJWlt9rbyDn92a1DqWOw9vWG2MFoays0sgObmWazO5BQPiFucnnEAjpAB+/Sw==" crossorigin="anonymous">
  
  
  <link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:400,700%7CMerriweather%7CRoboto+Mono">
  <link rel="stylesheet" href="/css/hugo-academic.css">
  

  
    <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-163319156-1', 'auto');
      ga('require', 'eventTracker');
      ga('require', 'outboundLinkTracker');
      ga('require', 'urlChangeTracker');
      ga('send', 'pageview');
    </script>
    <script async src="//www.google-analytics.com/analytics.js"></script>
    
    <script async src="https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js" integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin="anonymous"></script>
    
  

  <link rel="alternate" href="https://karttikeya.github.io/index.xml" type="application/rss+xml" title="Karttikeya Mangalam">
  <link rel="feed" href="https://karttikeya.github.io/index.xml" type="application/rss+xml" title="Karttikeya Mangalam">

  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/apple-touch-icon.png">

  <link rel="canonical" href="https://karttikeya.github.io/publication/plf/">

  

  <title>Disentangling Human Dynamics for Pedestrian Locomotion Forecasting with Noisy Supervision | Karttikeya Mangalam</title>

</head>
<body id="top" data-spy="scroll" data-target="#navbar-main" data-offset="71">

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="/">Karttikeya Mangalam</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      <ul class="nav navbar-nav navbar-right">
        

        

        <li class="nav-item">
          <a href="/#about">
            
            <span>Home</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/#publications_selected">
            
            <span>Publications</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/#projects">
            
            <span>Projects</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/#teaching">
            
            <span>Teaching</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/#contact">
            
            <span>Contact</span>
          </a>
        </li>

        
        

        
      </ul>

    </div>
  </div>
</nav>

<div class="pub" itemscope itemtype="http://schema.org/CreativeWork">

  


  <div class="container pub-title">
    <h1 itemprop="name">Disentangling Human Dynamics for Pedestrian Locomotion Forecasting with Noisy Supervision</h1>
    <span class="pub-authors" itemprop="author">
      
      <a href="http://karttikeya.github.io/" target="_blank">Karttikeya Mangalam</a>, <a href="https://web.stanford.edu/~eadeli/" target="_blank">Ehsan Adeli</a>, <a href="https://www.linkedin.com/in/kuan-hui-lee-23730370/" target="_blank">Kuan-Hui Lee</a>, <a href="https://www.linkedin.com/in/adrien-gaidon-63ab2358/" target="_blank">Adrien Gaidon</a>, <a href="http://www.niebles.net/" target="_blank">Prof. Juan Carlos Niebles</a>
      
    </span>
    <span class="pull-right">
      
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fkarttikeya.github.io%2fpublication%2fplf%2f"
         target="_blank">
        <i class="fa fa-facebook"></i>
      </a>
    </li>
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=Disentangling%20Human%20Dynamics%20for%20Pedestrian%20Locomotion%20Forecasting%20with%20Noisy%20Supervision&amp;url=https%3a%2f%2fkarttikeya.github.io%2fpublication%2fplf%2f"
         target="_blank">
        <i class="fa fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fkarttikeya.github.io%2fpublication%2fplf%2f&amp;title=Disentangling%20Human%20Dynamics%20for%20Pedestrian%20Locomotion%20Forecasting%20with%20Noisy%20Supervision"
         target="_blank">
        <i class="fa fa-linkedin"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=https%3a%2f%2fkarttikeya.github.io%2fpublication%2fplf%2f&amp;title=Disentangling%20Human%20Dynamics%20for%20Pedestrian%20Locomotion%20Forecasting%20with%20Noisy%20Supervision"
         target="_blank">
        <i class="fa fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=Disentangling%20Human%20Dynamics%20for%20Pedestrian%20Locomotion%20Forecasting%20with%20Noisy%20Supervision&amp;body=https%3a%2f%2fkarttikeya.github.io%2fpublication%2fplf%2f">
        <i class="fa fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


    </span>
  </div>

</div>
<div class="article-container">

  

  <h3>Abstract</h3>
  <p class="pub-abstract" itemprop="text">We tackle the problem of Human Locomotion Forecasting, a task for jointly predicting the spatial positions of several keypoints on human body in the near future under an egocentric setting. In contrast to the previous work that aims to solve either the task of pose prediction or trajectory forecasting in isolation, we propose a framework to unify these two problems and address the practically useful task of pedestrian locomotion prediction in the wild.  Among the major challenges in solving this task is the scarcity of annotated egocentric video datasets with dense annotations for pose, depth, or egomotion.   To surmount this difficulty,  we use state-of-the-art models to generate (noisy) annotations and propose robust forecasting models that can learn from this noisy supervision. We present a method to disentangle the overall pedestrian motion into easier to learn subparts by utilizing a pose completion and a decomposition module. The completion module fills in the missing key-point annotations and the decomposition module breaks the cleaned locomotion down to global (trajectory) and local (pose keypoint movements). Further, with Quasi RNN as our backbone, we propose a novel hierarchical trajectory forecasting network that utilizes low-level vision domain specific signals like egomotion and depth to predict the global trajectory. Our method leads to state-of-the-art results for the prediction of human locomotion in the egocentric view.</p>

  

  <div class="row">
    <div class="col-sm-1"></div>
    <div class="col-sm-10">
      <div class="row">
        <div class="col-xs-12 col-sm-3 pub-row-heading">Publication</div>
        <div class="col-xs-12 col-sm-9">Winter Conference on Applications of Computer Vision 2020</div>
      </div>
    </div>
    <div class="col-sm-1"></div>
  </div>
  <div class="visible-xs space-below"></div>

  <div class="row">
    <div class="col-sm-1"></div>
    <div class="col-sm-10">
      <div class="row">
        <div class="col-xs-12 col-sm-3 pub-row-heading">Date</div>
        <div class="col-xs-12 col-sm-9" itemprop="datePublished">
          March, 2020
        </div>
      </div>
    </div>
    <div class="col-sm-1"></div>
  </div>
  <div class="visible-xs space-below"></div>

  <div class="row" style="padding-top: 10px">
    <div class="col-sm-1"></div>
    <div class="col-sm-10">
      <div class="row">
        <div class="col-xs-12 col-sm-3 pub-row-heading" style="line-height:34px;">Links</div>
        <div class="col-xs-12 col-sm-9">

          




<a class="btn btn-primary btn-outline" href="https://arxiv.org/pdf/1911.01138.pdf">
  PDF
</a>


<a class="btn btn-primary btn-outline" href="https://karttikeya.github.io/pdf/plf_poster.pdf">
  Slides
</a>


<a class="btn btn-primary btn-outline" href="https://www.youtube.com/watch?v=GHkT0lyZtDg">
  Video
</a>







        </div>
      </div>
    </div>
    <div class="col-sm-1"></div>
  </div>
  <div class="visible-xs space-below"></div>

  <div class="space-below"></div>

  <div class="article-style">

<figure>
<center>
<iframe width="560" height="315" src="https://www.youtube.com/embed/GHkT0lyZtDg" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<figcaption>Oral Presentation Video (WACV 2020) </figcaption>
</center>
</figure>

<p><center> <h3> Qualitative Results </h3> </center>
<figure>
<center>
<iframe width="560" height="315" src="https://www.youtube.com/embed/-4gCHoSrWvY" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
</center>
<figcaption>Qualitative Results for overall locomotion forecasting in self-driving scenarios. Blue represents input locomotion to the network, Green is the predicted output locomotion. The network reasons jointly about path &amp; pose for plausible future locomotion predictions.  </figcaption></p>

<p></figure></p>

<h3 id="please-cite-using-following-bibtex">Please cite using following bibtex:</h3>

<pre><code>@inproceedings{mangalam2020disentangling,
  title={Disentangling human dynamics for pedestrian locomotion forecasting with noisy supervision},
  author={Mangalam, Karttikeya and Adeli, Ehsan and Lee, Kuan-Hui and Gaidon, Adrien and Niebles, Juan Carlos},
  booktitle={The IEEE Winter Conference on Applications of Computer Vision},
  pages={2784--2793},
  year={2020}
}
</code></pre>
</div>

</div>

<div class="container">
  <nav>
  <ul class="pager">
    
    <li class="previous"><a href="https://karttikeya.github.io/publication/future-localization/"><span
      aria-hidden="true">&larr;</span> Future Person Localization in First-Person Videos</a></li>
    

    
  </ul>
</nav>

</div>

<footer class="site-footer">
  <div class="container">
    <p class="powered-by">

      &copy; 2020 Karttikeya Mangalam &middot; 

      Powered by the <a href="https://github.com/gcushen/hugo-academic" target="_blank">Academic
      theme</a> for <a href="http://gohugo.io" target="_blank">Hugo</a>.

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>

    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/1.12.4/jquery.min.js" integrity="sha512-jGsMH83oKe9asCpkOVkBnUrDDTp8wl+adkB2D+//JtlxO4SrLoJdhbOysIFQJloQFD+C4Fl1rMsQZF76JjV0eQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.2/imagesloaded.pkgd.min.js" integrity="sha512-iHzEu7GbSc705hE2skyH6/AlTpOfBmkx7nUqTLGzPYR+C1tRaItbRlJ7hT/D3YQ9SV0fqLKzp4XY9wKulTBGTw==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/gsap/1.19.1/TweenMax.min.js" integrity="sha512-Z5heTz36xTemt1TbtbfXtTq5lMfYnOkXM2/eWcTTiLU01+Sw4ku1i7vScDc8fWhrP2abz9GQzgKH5NGBLoYlAw==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/gsap/1.19.1/plugins/ScrollToPlugin.min.js" integrity="sha512-CDeU7pRtkPX6XJtF/gcFWlEwyaX7mcAp5sO3VIu/ylsdR74wEw4wmBpD5yYTrmMAiAboi9thyBUr1vXRPA7t0Q==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha512-iztkobsvnjKfAtTNdHkGVjAYTrrtlC7mGp/54c40wowO7LhURYl3gVzzcEqGl/qKXQltJ2HwMrdLcNUdo+N/RQ==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.4/isotope.pkgd.min.js" integrity="sha512-VDBOIlDbuC4VWxGJNmuFRQ0Li0SKkDpmGyuhAG5LTDLd/dJ/S0WMVxriR2Y+CyPL5gzjpN4f/6iqWVBJlht0tQ==" crossorigin="anonymous"></script>
    
    <script src="/js/hugo-academic.js"></script>
    

    
    
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.9.0/highlight.min.js"></script>

      

      

      <script>hljs.initHighlightingOnLoad();</script>
    

    
    

  </body>
</html>

