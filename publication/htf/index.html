
<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
    body {
        font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
        font-weight:300;
        font-size:18px;
        margin-left: auto;
        margin-right: auto;
        width: 1100px;
    }

    h1 {
        font-weight:300;
    }

    .disclaimerbox {
        background-color: #eee;
        border: 1px solid #eeeeee;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
        padding: 20px;
    }

    video.header-vid {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }

    img.header-img {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }

    img.rounded {
        border: 1px solid #eeeeee;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }

    a:link,a:visited
    {
        color: #1367a7;
        text-decoration: none;
    }
    a:hover {
        color: #208799;
    }

    td.dl-link {
        height: 160px;
        text-align: center;
        font-size: 22px;
    }

    tr.spaceUnder>td {
        padding-bottom: 10px;
    }
    .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
                0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
                5px 5px 0 0px #fff, /* The second layer */
                5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
                10px 10px 0 0px #fff, /* The third layer */
                10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
                15px 15px 0 0px #fff, /* The fourth layer */
                15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
                20px 20px 0 0px #fff, /* The fifth layer */
                20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
                25px 25px 0 0px #fff, /* The fifth layer */
                25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
        margin-left: 10px;
        margin-right: 45px;
    }


    .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
                0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
                5px 5px 0 0px #fff, /* The second layer */
                5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
                10px 10px 0 0px #fff, /* The third layer */
                10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
        margin-top: 5px;
        margin-left: 10px;
        margin-right: 30px;
        margin-bottom: 5px;
    }

    .vert-cent {
        position: relative;
        top: 50%;
        transform: translateY(-50%);
    }

    hr
    {
        border: 0;
        height: 1.5px;
        background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
    }
</style>

<html>
  <head>
        <title>It Is Not the Journey but the Destination: Endpoint Conditioned Trajectory Prediction</title>
        <meta property="og:title" content="sceneflow" />
        <!-- <meta property="og:url" content="https://www.youtube.com/watch?v=cYHQKtBLI3Q" /> -->
  </head>

  <body>
    <br>
    <center>
    <span style="font-size:42px">It Is Not the Journey but the Destination: <br> Endpoint Conditioned Trajectory Prediction </span>


    <!-- </center> -->
    
    <br>
    <br>
      <table align=center width=800px>

      <!--
       <tr>
         <td align=center width=100px>
         <center>
         <span style="font-size:16px"><a href="https://people.eecs.berkeley.edu/~zhecao/">Zhe Cao</a></span><sup>1</sup>
         </center>
         </td>

        <td align=center width=100px>
        <center>
        <span style="font-size:16px"><a href="https://people.eecs.berkeley.edu/~hangg/">Hang Gao</a></span><sup>1</sup>
        </center>
        </td>

        <td align=center width=150px>
        <center>
        <span style="font-size:16px"><a href="https://karttikeya.github.io/">Karttikeya Mangalam</a></span><sup>1</sup>
        </center>
        </td>

        <td align=center width=100px>
        <center>
        <span style="font-size:16px"><a href="https://scholar.google.com/citations?user=oyh-YNwAAAAJ&hl=en">Qi-Zhi Cai</a></span><sup>2</sup>
        </center>
        </td>

        <td align=center width=100px>
        <center>
        <span style="font-size:16px"><a href="https://minhpvo.github.io/">Minh Vo</a></span><sup>3</sup>
        </center>
        </td>

        <td align=center width=100px>
        <center>
        <span style="font-size:16px"><a href="http://www.eecs.berkeley.edu/~malik/">Jitendra Malik</a></span><sup>1</sup>
        </center>
        </td>
     </tr>
   -->

     <tr>
       <span style="font-size:22px"><a href="https://karttikeya.github.io/">Karttikeya Mangalam</a></span><sup>1</sup>,&nbsp;&nbsp;
      <span style="font-size:22px"><a href="https://www.linkedin.com/in/harshayu-girase-764b06153">Harshayu Girase</a></span><sup>1</sup>,&nbsp;&nbsp;
      <span style="font-size:22px"><a href="https://www.linkedin.com/in/shreyas-agarwal-086267146">Shreyas Agarwal</a></span><sup>1</sup>,&nbsp;&nbsp;
      <span style="font-size:22px"><a href="https://www.linkedin.com/in/kuan-hui-lee-23730370">Kuan-Hui Lee</a></span><sup>2</sup>,&nbsp;&nbsp;
      <span style="font-size:22px"><a href="https://web.stanford.edu/~eadeli/">Ehsan Adeli</a></span><sup>3</sup><br>
      <span style="font-size:22px"><a href="https://people.eecs.berkeley.edu/~malik/">Prof. Jitendra Malik</a></span><sup>1</sup>, &nbsp;&nbsp;
      <span style="font-size:22px"><a href="https://www.linkedin.com/in/adrien-gaidon-63ab2358/">Adrien Gaidon</a></span><sup>2</sup>
   </tr>


     <tr>
       <td align=center colspan="2" style="font-size:22px">
       <center>
       <sup>1</sup>UC Berkeley
       </center>
       </td>

      <td align=center colspan="2" style="font-size:22px">
      <center>
      <sup>2</sup>Toyota Research Institute
      </center>
      </td>

      <td align=center colspan="2" style="font-size:22px">
      <center>
      <sup>3</sup>Stanford University
      </center>
      </td>
      </tr>
    </table>

        <br> <br>
    <table align=center width=700px>
       <tr>
        <td align=center width=100px>
        <center>
        <span style="font-size:28px">ECCV 2020 <span style="color:red">(Oral)</span></span>
        </center>
        </td>
     </tr>
    </table>
         <table align=center width=900>
          <tr>
                <br><br>
                 <span style="font-size:28px">
                <a href="https://arxiv.org/pdf/2004.02025.pdf">[Paper]</a> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
                <a href="bibtex.txt">[Bibtex]</a> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 
                [Github] (Code Coming Soon)
              </span>
            <!-- [hosted on <a href="#">arXiv</a>]</a> -->
              </td>
        </tr>
      </table>
            <!--
            <br>
            <table align=center width=600px>
                <tr>
                    <td width=700px>
                      <center>
                          <img src = "teaser.jpg" height="180px"></img><br>
                    </center>
                    </td>
                </tr>

                <tr>
                  <td colspan="3"> <br>
                In the first image, can you predict what the human is going to do next? Depending on his intention, the person can choose to turn left to climb up stairs; he may also go straight through the hallway or turn right to fetch some items off the table. In this work, we propose a method to generate long-term stochastic predictions of future 3D human motion, while also considering the scene context.
    Given one single RGB image and 2D human pose history, our method first generates multiple possible future 2D destinations, then predicts future 3D human paths towards each destination, as shown in the middle, and finally generates 3D human pose sequences following the path, shown in the ground-truth 3D point cloud of the scene in the right. We train our model on both real-world data with noisy ground-truth and our newly-created large-scale synthetic data with diverse scenes, characters, and motions. Both quantitative comparisons and qualitative results demonstrate that our method can generate plausible scene-adaptive predictions.
                  </td>
                </tr>
            </table>


          <hr> -->
<!--           <br>
          <br> -->
<!--           <center>
            <table align=center width=700>
                <tr>
              
                  <td><video width="700px" controls> <source src="hmp.m4v" type=video/mp4><video></td>
              
              </tr>
            </table>
          </center> -->
          
          <br>

          <!-- <hr> -->
          <center><h1>Abstract</h1></center>
          <table align=center width=1000px>
              <tr>
                  <td width=1000px>
<!--                     <center>
                        <img src = "teaser.jpg" height="500px"></img><br>
                  </center> -->
                  <br>
                  <span style="font-size:20px"> Human trajectory forecasting with multiple socially interacting agents is of critical importance for autonomous navigation in human
environments, e.g., for self-driving cars and social robots. In this work, we
present Predicted Endpoint Conditioned Network (PECNet) for flexible
human trajectory prediction. PECNet infers distant trajectory endpoints
to assist in long-range multi-modal trajectory prediction. A novel nonlocal social pooling layer enables PECNet to infer diverse yet socially compliant trajectories. Additionally, we present a simple “truncation trick” for improving few-shot multi-modal trajectory prediction performance. We show that PECNet improves state-of-the-art performance on the Stanford Drone prediction benchmark by ∼19.5% & on the ETH/UCY benchmark by ∼40.8%.
                  </td>
              </tr>

              <tr>
                <td colspan="3"> <br>

                </td>
              </tr>
          </table>


          <hr>
          <center><h1>Key Ideas</h1></center>
          <table align=center width=600px>
              <tr>
                  <td width=700px>
                    <center>
                        <img src = "teaser.JPG" height="500px"></img><br>
                  </center>
                  <br>
                  <span style="font-size:18px"> <b>Imitating the Human Path Planning Process</b>: We posit that pedestrians in the scene move towards a predetermined position and interactions such as social signalling shape their trajectories only locally while they still go along achieving their original intention. Instantiating this idea, we propose 
                  to model the pedestrian trajectory prediction problem (top left) by breaking down the task in two sequential steps that are learned end to end. (a) Inferring the local endpoint distribution (top right) for diverse endpoint sampling for each agent independently; and then (b) Conditioning on sampled future endpoints (bottom left) for planning socially compliant trajectories for all the agents in the scene jointly (bottom right).
                  </td>
              </tr>
<!-- 
We posit that pedestrians in the scene move towards a predetermined position and interactions such as social cues happen as they go along achieving this intention without changing the predilection while still shaping their trajectories locally. 
               -->
              <tr>
                <td colspan="3"> <br>

                </td>
              </tr>
          </table>


        <hr>
         <!-- <table align=center width=550px> -->
                <center><h1>Multimodal Results</h1></center>
                <table align=center width=900px>
                    <tr>
                        <td width=1200px colspan="3">
                          <center>
                              <img src = "multimodal.png" height="540px"></img><br>
                        </center>
                        </td>
                    </tr>
<!--                     <tr>
                      <td align=center width=400px>
                        (a)
                      </td>
                      <td align=center width=400px>
                        (b)
                      </td>
                      <td align=center width=400px>
                        (c)
                      </td>
                  </tr> -->

                    <tr>
                        <td width=600px colspan="3">
                                            <br>
                              <span style="font-size:18px"><b> Visualizing Multimodality Predictions:</b> Qualitative results for diverse multi-modal predictions produced by PECNet on the <a href="https://cvgl.stanford.edu/projects/uav_data/"> Stanford Drone Dataset</a>. White represents the past 3.2 seconds trajectory (8 frames) while  red  &  cyan  represents  predicted  &  ground  truth  future  respectively  over  next 4.8 seconds (12 frames). As demonstrated, PECNet predictions capture a wide-range of plausible trajectory behaviours while discarding improbable ones such as endpoints incompatible with the direction of motion. 
                        </td>
                    </tr>
                </table>
                <br>
      <hr>
      <table align=center width=800>
       <center><h1>Socially Compliant Diverse Predictions</h1></center>
          <tr class="spaceUnder">
            <td>
              <img style="width:280px" src="gt2.gif"/>
            </td>
            <td>
              <img style="width:280px" src="pred2.gif"/>
            </td>
            <td>
            <img style="width:280px" src="viz2.gif"/>
          </td>
        </tr>

        <tr>
          <td>
            <img style="width:280px" src="gt3.gif"/>
          </td>
          <td>
            <img style="width:280px" src="pred3.gif"/>
          </td>
          <td>
            <img style="width:280px" src="viz3.gif"/>
          </td>
      </tr>

        <tr>
          <td align=center><br>
            <span style="font-size:24px">&nbsp;<a href='#'>Ground Truth</a>
          </td>
          <td align=center><br>
            <span style="font-size:24px">&nbsp;<a href='#'>Final Predictions</a>
          </td>
          <td align=center><br>
            <span style="font-size:24px">&nbsp;<a href='#'>Multimodal Predictions</a>
          </td>
        <tr>
        <tr>
          <td colspan="3"> <br>
            <span style="font-size:18px"><b> Qualitative Results at Mergers & Intersection:</b> We demonstrate PECNet's socially compliant & diverse trajectories in multi-agent settings in tricky scenarios such as path merger (top row) or collision avoidance at lane intersections (bottom row). The left column denotes the ground truth trajectories from Stanford Drone Dataset and the middle and left columns denote our predictions. For ground truth/final predictions, circles denote the past input fed into PECNet, while stars denote the future to be predicted/predicted with tails denoting the last four observed positions for both. Our "best" predictions follow the ground truth closely while effectively avoid collisions with other pedestrians in a natural seamless way. While PECNet's multimodal predictions produces diverse socially compliant trajectories jointly for all the pedestrians in the scene (extended temporally for visualization using recurrent prediction). For quantitative results please see <a href="https://arxiv.org/pdf/2004.02025.pdf">our paper</a>.  
          </td>
        </tr>
      </table>
    <br>

    <hr>
      <table align=center width=900>
       <center><h1>Paper</h1></center>
          <tr>
            <td><a href="https://arxiv.org/pdf/2004.02025.pdf"><img align=center style="width:400px" src="paper_collage.png"/></a></td>
            <td><span style="font-size:14pt">Mangalam, Girase, Agarwal, Lee, <br> Adeli, Malik, Gaidon.<br><br>
              It Is Not the Journey but the Destination: Endpoint Conditioned Trajectory Prediction<br><br>
            ECCV 2020  <a href = "https://eccv2020.eu/"> <b> (Oral) </b> </a><br><br>
                <a href="https://arxiv.org/pdf/2004.02025.pdf">[Paper]</a> &nbsp; &nbsp;
                <a href="bibtex.txt">[Bibtex]</a> &nbsp; &nbsp;
                [Github] (Code Coming Soon)
            <!-- [hosted on <a href="#">arXiv</a>]</a> -->
              </td>
        </tr>
      </table>
    <br>
    <!--
    <hr>
      <center><h1>Code</h1></center>
      <tr>
        <td>
          <span style="font-size:28px">&nbsp;<a href='#'>[GitHub]</a>  (coming soon)
        </td>
      <br>
      -->
      <hr>
            <table align=center width=950px>
                <tr>
                    <td>
                      <left>
                <center><h1>Acknowledgements</h1></center>
                We thank <span style="font-size:14px"><a href="http://www.niebles.net/">Prof. Juan Carlos Niebles</a></span> for helpful advice and suggestions. This webpage template was borrowed from some <a href="https://richzhang.github.io/colorization/">colorful folks</a>.
            </left>
        </td>
        </tr>
        </table>

        <br><br>
</body>
</html>
